{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Annotator Agreement Calculation\n",
    "\n",
    "This notebook contains the code required to calculate the IAA scores as shown in the paper, evaluated on a small subset of the original dataset, with agreement calculated on the dates, phrases, classes and date-event relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "1. [Date Agreement](#date_agreement)\n",
    "2. [Event Class and Phrase Agreement](#phrase_agreement)\n",
    "3. [Relation Agreement](#relation_agreement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"date_agreement\"><h2>Agreement on dates</h2></a>\n",
    "\n",
    "The first part of the inter-annotator agreement measurements is the calculation of Cohen's kappa for the dates, to see the agreement over dates extracted from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torchmetrics.text.rouge import ROUGEScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def align_annotation_files(filename_1: str, filename_2: str) -> [pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Function that loads the annotation files, and matches them up with the documents \n",
    "    that were annotated by both annotators.\n",
    "    \n",
    "    returns: (df1, df2) containing two two matched dataframes for both annotators.\n",
    "    \"\"\"\n",
    "    annot_df_1 = pd.read_csv(filename_1)\n",
    "    annot_df_2 = pd.read_csv(filename_2)\n",
    "    \n",
    "    sort_cols = ['doc_id', 'start'] if 'start' in annot_df_1.columns else ['doc_id']\n",
    "    \n",
    "    annot_df_1 = annot_df_1.sort_values(sort_cols)\n",
    "    annot_df_2 = annot_df_2.sort_values(sort_cols)\n",
    "    \n",
    "    matched_documents = set(annot_df_1['doc_id']) & set(annot_df_2['doc_id'])\n",
    "\n",
    "    annot_df_1 = annot_df_1[annot_df_1['doc_id'].isin(matched_documents)]\n",
    "    annot_df_2 = annot_df_2[annot_df_2['doc_id'].isin(matched_documents)]\n",
    "\n",
    "    return annot_df_1, annot_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotation files for both annotators\n",
    "dates_df_1, dates_df_2 = align_annotation_files('annotator1/entities.csv', 'annotator2/entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected the dates from the sentences, which are contained in the 'text' column\n",
    "aligned_dates_annot_1 = dates_df_1['text']\n",
    "aligned_dates_annot_2 = dates_df_2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens_kappa_dates = cohen_kappa_score(aligned_dates_annot_1, aligned_dates_annot_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cohens Kappa for the 334 aligned dates is 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"The cohens Kappa for the %d aligned dates is %.2f\" % (aligned_dates_annot_1.shape[0], choens_kappa_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the Cohens kappa for the extraction of the date is perfect. This is due to the fact that these dates are extracted form the text directly, and therefore can be easily identified and extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"phrase_agreement\"><h2>Agreement on Events Classes and Phrases</h2></a>\n",
    "\n",
    "After calculating the agreement over the dates, the agreement over de events and their classes is calculated. For the event phrases, we do this both using exact matches, as well as calculating the rouge-L score to allow for some small annotation differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cohen's kappa for the 26 events is 0.91\n",
      "The cohen's kappa for the 26 event phrases is 0.68\n",
      "The average Rouge-L overlap between event phrases is 0.86\n"
     ]
    }
   ],
   "source": [
    "event_df_1, event_df_2 = align_annotation_files('annotator1/date_event_combinations.csv', 'annotator2/date_event_combinations.csv')\n",
    "\n",
    "# Event class Kappa score\n",
    "print(\"The cohen's kappa for the %d events is %.2f\" % (event_df_1.shape[0], cohen_kappa_score(event_df_1['label'], event_df_2['label'])))\n",
    "annotator_1_event_phrases = event_df_1['event'].str.strip().str.lower()\n",
    "annotator_2_event_phrases = event_df_2['event'].str.strip().str.lower()\n",
    "\n",
    "print(\"The cohen's kappa for the %d event phrases is %.2f\" % (event_df_1.shape[0], cohen_kappa_score(annotator_1_event_phrases, annotator_2_event_phrases)))\n",
    "def calculate_rouge_overlap(ser1: pd.Series, ser2: pd.Series)-> float:\n",
    "    rouge = ROUGEScore(use_stemmer=True, tokenizer=word_tokenize)\n",
    "    rouge_scores = pd.Series([rouge(phrase_1, phrase_2)['rougeL_fmeasure'].item() for phrase_1, phrase_2 in zip(ser1, ser2)])\n",
    "    return rouge_scores.mean()\n",
    "\n",
    "print(\"The average Rouge-L overlap between event phrases is %.2f\" % calculate_rouge_overlap(annotator_1_event_phrases, annotator_2_event_phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"relation_agreement\"><h2>Agreement on Relations</h2></a>\n",
    "\n",
    "Apart from the agreement on the dates and the event classes, there is also the agreement on the relations, i.e. whether an event description consists of one or two parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relation_df_1, relation_df_2 = align_annotation_files('annotator1/relations.csv', 'annotator2/relations.csv')\n",
    "\n",
    "def get_relation_ids(relations: list)-> list:\n",
    "    ids = [eval(event_list) for event_list in relations]\n",
    "    return ids\n",
    "\n",
    "annot_1_event_ids = get_relation_ids(event_df_1['event_ids'].values)\n",
    "annot_2_event_ids = get_relation_ids(event_df_2['event_ids'].values)\n",
    "\n",
    "def get_relations(event_ids, df):\n",
    "    relations = []\n",
    "    for i in event_ids:\n",
    "        relation = ''\n",
    "        for id in i:\n",
    "            type = df.loc[df['to_id'] == id]['type'].values[0]\n",
    "            relation += type\n",
    "        relations.append(relation)\n",
    "    return relations    \n",
    "\n",
    "relation_types_1 = get_relations(annot_1_event_ids, relation_df_1)\n",
    "relation_types_2 = get_relations(annot_2_event_ids, relation_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's kappa for agreeing on labeling relations correctly: 0.62\n"
     ]
    }
   ],
   "source": [
    "print(\"Cohen's kappa for agreeing on labeling relations correctly:\", round(sklearn.metrics.cohen_kappa_score(relation_types_1, relation_types_2), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_redaction_env",
   "language": "python",
   "name": "text_redaction_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
